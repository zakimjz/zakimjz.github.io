<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>CSCI4969-6969 Assign3  | Zaki Home Page</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="http://www.cs.rpi.edu/~zaki/courses/mlib/mlib_assign3/">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><meta name="author" content="Mohammed J. Zaki">
<meta property="og:site_name" content="Zaki Home Page">
<meta property="og:title" content="CSCI4969-6969 Assign3 ">
<meta property="og:url" content="http://www.cs.rpi.edu/~zaki/courses/mlib/mlib_assign3/">
<meta property="og:description" content="Attention for Secondary Structure Prediction
Due: March 6th, before midnight
In this assignment, you will implement one transformer block, i.e.,
applying attention over the input protein sequence to p">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2020-03-30T09:21:31-04:00">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="../../../">

            <span id="blog-title">Zaki Home Page</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../../publications/" class="nav-link">Publications</a>
                </li>
<li class="nav-item">
<a href="https://github.com/zakimjz" class="nav-link">Github</a>
                </li>
<li class="nav-item">
<a href="../../datamining" class="nav-link">DataMining</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">CSCI4969-6969 Assign3 </a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div class="section" id="attention-for-secondary-structure-prediction">
<h2>Attention for Secondary Structure Prediction</h2>
<p>Due: March 6th, before midnight</p>
<p>In this assignment, you will implement one transformer block, i.e.,
applying attention over the input protein sequence to predict the
secondary structure at each position.</p>
<p>We will use the same dataset as for Assign2. Namely, the
<a class="reference external" href="http://ww.cs.rpi.edu/~zaki/MLIB/train_SS.txt">http://ww.cs.rpi.edu/~zaki/MLIB/train_SS.txt</a> (training dataset)
comprises 5365 protein sequences, along with the secondary structure
(SS) label at each residue position. Each line in the file contains a
protein sequence, followed by a space, followed by the label sequence
(there is a one-to-one correspondence between an amino acid and the SS
label). Likewise the <a class="reference external" href="http://ww.cs.rpi.edu/~zaki/MLIB/test_SS.txt">http://ww.cs.rpi.edu/~zaki/MLIB/test_SS.txt</a> (test
dataset) comprises 514 sequences, along with the true labels, in the
above format.</p>
<p>Given an input sequence $x_1, x_2, ..., x_n$, where each $x_i$ is a
$d$-dimensional vector (e.g., pretrained vector, or a one-hot vector)
over a suitable vocabulary (e.g., single amino acids, or ngram words of
size 3, 5, etc), you will predict the corresponding SS label for that
position.</p>
<p>You are required to implement the transformer block that considers the
pair-wise attention scores between the center word, and other words
within a block $w$. For this, you should use the key $K$, query
$Q$, and value $V$ representations (in a lower dimensional subspace)
that rely on the corresponding projection matrices $W^K, W^Q, W^V$.
You must write the code that computes the attention and then the final
value representation for each word, which will then be mapped back to
the original dimensionality $d$, via another matrix $W^O$.</p>
<p>After the attention update, we can then pass the word vectors through a
feed-forward network (FFN with relu activation), followed by a softmax
output layer for each position in the input. You will then measure the
cross-entropy loss per position, and sum them to compute the final loss
for the given input sequence.</p>
<p>Note that the parameters $W^K, W^Q, W^V, W^O$ and the parameters for
the FFN and softmax are all shared!</p>
<p>Note that the input is given as an amino acid sequence, so depending on
the word (or ngram) size, you first have to convert it into a sequence
of words using a sliding window, so you can look up the vector
representation. Second, the block size $w$ is like the context size,
in that you should be looking at the attention between the center words,
and $w$ words to the left and $w$ to the right.</p>
<p>Once you have implemented the basic attention module, you may play with
more advanced variations like multiple attention layers, residual
connections, and layer normalization.</p>
<div class="section" id="submission">
<h3>Submission</h3>
<p>Submit <strong>assign3.py</strong> via submitty, along with an output file that
summarizes the results of your method in terms of training and testing
accuracy values.</p>
</div>
</div>
    </div>
    
        
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article><!--End of body content--><footer id="footer">
            Contents Â© 2020         <a href="mailto:zaki@cs.rpi.edu">Mohammed J. Zaki</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-11058802-1"></script><script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-11058802-1');
</script>
</body>
</html>
