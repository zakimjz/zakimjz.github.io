<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>CSCI4949-6969 Assign4 | Zaki Home Page</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="http://www.cs.rpi.edu/~zaki/courses/mlib/mlib_assign4/">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><meta name="author" content="Mohammed J. Zaki">
<meta property="og:site_name" content="Zaki Home Page">
<meta property="og:title" content="CSCI4949-6969 Assign4">
<meta property="og:url" content="http://www.cs.rpi.edu/~zaki/courses/mlib/mlib_assign4/">
<meta property="og:description" content="Assign4: Protein Contextual Embeddings via BERT
Due Date: Mar 4, before midnight (EST 11:59:59PM)
In this assignment, you will implement the BERT embedding method for protein
sequences.
Your code shou">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-02-25T19:00:31-04:00">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark
bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="../../../">

            <span id="blog-title">Zaki Home Page</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../../publications/" class="nav-link">Publications</a>
                </li>
<li class="nav-item">
<a href="https://github.com/zakimjz" class="nav-link">Github</a>
                </li>
<li class="nav-item">
<a href="../../mlib" class="nav-link">MLinBioinfo</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">CSCI4949-6969 Assign4</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <h2>Assign4: Protein Contextual Embeddings via BERT</h2>
<p><strong>Due Date</strong>: Mar 4, before midnight (EST 11:59:59PM)</p>
<p>In this assignment, you will implement the BERT embedding method for protein
sequences.</p>
<p>Your code should implement on your own the transformer approach with masked
language model for learning contextual embeddings for input sequences. You
can use the pytorch layer norm and feed forward layers, but you must
implement the multi-head attention on your own. The details of BERT training
can be found in <a href="https://arxiv.org/abs/1810.04805">BERT</a>, and those for the
transformer in <a href="https://arxiv.org/abs/1706.03762">Transformer</a>. You must
implement at least one transformer block, but you may play with the full
model too (12 layers!). You have to also implement the masked language
model.</p>
<p>For training  the BERT model, you will use the same large set of 524532 protein sequences
<a href="http://www.cs.rpi.edu/~zaki/MLIB/data/uniprot-reviewed-lim_sequences.txt">uniprot-reviewed-lim_sequences.txt</a>. This data is from the paper
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6061698">Learned
protein embeddings for machine learning</a>.
For prototyping the algorithm you can make use of the dataset of 1000 proteins
<a href="http://www.cs.rpi.edu/~zaki/MLIB/data/small_uniprot.txt">small_uniprot.txt</a>. </p>
<p>One complication in BERT training is what tokenization to use. You can try
the basic character-level tokenization, i.e., each amino acid is a token,
i.e., kmer=1. On the training dataset, the max sequence length is 999, so
you should use a block size of 1000, with the first token being "CLS". </p>
<p>If you are feeling ambitious, you an also try a kmer-3 tokenization, in
which case, each sequence can either be represented via a sliding window of
all kmers, or you can create three different sequences based on the starting
offset, and use non-overlapping kmers for pre-training.</p>
<h2>Evaluation</h2>
<p>Once you have pre-trained the BERT model, you will evaluate it on the
protein family classification described in the paper <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287">Continuous Distributed
Representation of Biological Sequences for Deep Proteomics and
Genomics</a>.
The dataset is available online at
<a href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JMFHTN">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/JMFHTN</a>.
In particular, use the testing sequences in the file
"family_classification_sequences.csv". This file has 324018 sequences from
different protein families. The true class for these sequences is listed in
the last column of the file "family_classification_metadata.csv". </p>
<p>For each family, you should as positive class the sequences that have the
given family label, and then you should select an equal number of random
sequences from the whole dataset as negative class to train an MLP model for
binary classification. The MLP model will use the pre-trained BERT model to
learn a representation for each example sequence in the dataset.</p>
<p>Finally, you should also compare the benefit of BERT over ProtVec by using
the pre-trained word2vec based embeddings in the "protVec_100d_3grams.csv"
file. In addition, you may also use your own prot2vec embeddings from the
previous assignment if you want.</p>
<h2>Submission</h2>
<p>Submit your jupyter notebooks (or python scripts) via submitty. </p>
<p>The first notebook is for the BERT model.
The output of your code should the pre-trained BERT model for the
pre-training code.  </p>
<p>The second notebook is for the classification.
Next, you need to report the classification accuracy
using the pre-trained BERT model, as well as the prot2vec model on the
protein family classification dataset. Report the classification results
(specificity, sensitivity, and accuracy) for the first 1000, 2000, 3000,
and 4000 families, as well as for the whole dataset, as shown in Table 2
in the paper.</p>
<p>You should acknowledge the source of any code you use from the web.</p>
    </div>
    
        
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article><!--End of body content--><footer id="footer">
            Contents Â© 2022         <a href="mailto:zaki@cs.rpi.edu">Mohammed J. Zaki</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-11058802-1"></script><script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-11058802-1');
</script>
</body>
</html>
