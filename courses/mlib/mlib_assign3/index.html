<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>CSCI4949-6969 Assign3 | Zaki Home Page</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../../../rss.xml">
<link rel="canonical" href="http://www.cs.rpi.edu/~zaki/courses/mlib/mlib_assign3/">
<!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><!-- Font Awesome --><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.3/css/all.css" integrity="sha384-UHRtZLI+pbxtHCWp1t77Bi1L4ZtiqrqD80Kn4Z8NTSRyMA2Fd33n5dQ8lWUE00s/" crossorigin="anonymous">
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><meta name="author" content="Mohammed J. Zaki">
<meta property="og:site_name" content="Zaki Home Page">
<meta property="og:title" content="CSCI4949-6969 Assign3">
<meta property="og:url" content="http://www.cs.rpi.edu/~zaki/courses/mlib/mlib_assign3/">
<meta property="og:description" content="Assign3: Protein Embeddings
Due Date: Feb 22, before midnight (EST 11:59:59PM)
In this assignment, you will implement the ProtVec embedding method
described in the paper
Continuous Distributed Represe">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2022-02-11T19:00:31-04:00">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark
bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="../../../">

            <span id="blog-title">Zaki Home Page</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../../publications/" class="nav-link">Publications</a>
                </li>
<li class="nav-item">
<a href="https://github.com/zakimjz" class="nav-link">Github</a>
                </li>
<li class="nav-item">
<a href="../../datamining" class="nav-link">DataMining</a>
                </li>
<li class="nav-item">
<a href="../../mlib" class="nav-link">MLinBioinfo</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">CSCI4949-6969 Assign3</a></h1>

        

    </header><div class="e-content entry-content" itemprop="articleBody text">
    <h2>Assign3: Protein Embeddings</h2>
<p><strong>Due Date</strong>: Feb 22, before midnight (EST 11:59:59PM)</p>
<p>In this assignment, you will implement the ProtVec embedding method
described in the paper
<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287">Continuous Distributed Representation of Biological Sequences for Deep
Proteomics and Genomics</a></p>
<p>Your code should implement the negative sampling approach to train the
embeddings. For training  the model, you can first use a small set of 1000 proteins
<a href="http://www.cs.rpi.edu/~zaki/MLIB/data/small_uniprot.txt">small_uniprot.txt</a>. 
Once your model is finalized you should train
it on the large set of 524532 protein sequences
<a href="http://www.cs.rpi.edu/~zaki/MLIB/data/uniprot-reviewed-lim_sequences.txt">uniprot-reviewed-lim_sequences.txt</a>. This data is from the paper
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6061698">Learned
protein embeddings for machine learning</a>.</p>
<p>You should compare with the default values used in the paper, namely
embedding dimensionality $d=100$, window size $w=25$, and k-mer/n-gram size $k=3$, and the number of negative samples per positive example $q=5$. </p>
<p>Here is the pseudo code for the overall method:</p>
<pre class="code literal-block"><span></span><code>   <span class="n">create</span> <span class="n">the</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">probability</span> <span class="n">distribution</span><span class="p">,</span> <span class="ow">and</span> <span class="n">word</span> <span class="n">to</span> <span class="n">index</span> <span class="p">(</span><span class="ow">and</span> <span class="n">reverse</span> <span class="n">mappings</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="n">k</span><span class="o">-</span><span class="n">mer</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">sequence</span> <span class="n">at</span> <span class="n">each</span> <span class="n">of</span> <span class="n">the</span> <span class="n">offsets</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">2.</span>

   <span class="n">write</span> <span class="n">a</span> <span class="n">function</span> <span class="n">to</span> <span class="k">return</span> <span class="n">a</span> <span class="n">batch</span> <span class="n">of</span> <span class="n">positive</span> <span class="ow">and</span> <span class="n">negative</span> <span class="n">pairs</span> <span class="kn">from</span> <span class="nn">all</span> <span class="n">of</span> <span class="n">the</span> <span class="n">sequences</span><span class="o">/</span><span class="n">offsets</span><span class="o">.</span>
   <span class="k">for</span> <span class="n">the</span> <span class="n">negative</span> <span class="n">sampling</span> <span class="n">use</span> <span class="n">the</span> <span class="n">cumsum</span> <span class="n">approach</span> <span class="n">to</span> <span class="n">sample</span> <span class="n">random</span> <span class="n">words</span> 

   <span class="n">define</span> <span class="n">NN</span> <span class="n">model</span><span class="p">:</span>
       <span class="n">init</span> <span class="n">function</span><span class="p">:</span>
           <span class="n">define</span> <span class="n">the</span> <span class="n">two</span> <span class="n">embeddings</span> <span class="n">layers</span> <span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

       <span class="n">forward</span> <span class="n">function</span><span class="p">:</span>
           <span class="nb">input</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">batch</span> <span class="n">of</span> <span class="n">target_words</span><span class="p">,</span> <span class="ow">and</span> <span class="n">context_words</span> 
           <span class="n">look</span> <span class="n">up</span> <span class="n">their</span> <span class="n">embeddings</span>
           <span class="n">compute</span> <span class="n">the</span> <span class="n">dot</span> <span class="n">product</span> <span class="n">between</span> <span class="n">corresponding</span> <span class="n">pairs</span>
           <span class="n">output</span> <span class="n">should</span> <span class="n">be</span> <span class="n">the</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">that</span> <span class="n">pair</span> <span class="n">being</span> <span class="n">a</span> <span class="n">positive</span> <span class="n">pair</span> <span class="p">(</span><span class="n">via</span> <span class="n">sigmoid</span><span class="p">),</span>
           <span class="ow">or</span> <span class="n">leave</span> <span class="n">it</span> <span class="k">as</span> <span class="n">logits</span>

   <span class="n">Next</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">boilerplate</span> <span class="n">code</span> <span class="k">for</span> <span class="n">training</span><span class="p">:</span>
   <span class="n">net</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
   <span class="n">send</span> <span class="n">net</span> <span class="n">to</span> <span class="n">GPU</span>
   <span class="n">loss_func</span> <span class="o">=</span> <span class="n">BCE</span> <span class="n">loss</span> <span class="p">(</span><span class="k">with</span> <span class="n">logits</span><span class="p">)</span>
   <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span>

   <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">epochs</span>
       <span class="k">for</span> <span class="n">target_words</span><span class="p">,</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">batches</span>
          <span class="n">send</span> <span class="n">batch</span> <span class="n">data</span> <span class="n">to</span> <span class="n">the</span> <span class="n">GPU</span>
          <span class="n">net</span><span class="o">.</span><span class="n">zero_grad</span>
          <span class="n">preds</span> <span class="o">=</span> <span class="n">net</span> <span class="p">(</span><span class="n">target_words</span><span class="p">,</span> <span class="n">contex_words</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
          <span class="n">loss</span><span class="o">.</span><span class="n">backward</span>
          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span>

      <span class="nb">print</span> <span class="n">total</span> <span class="n">loss</span> <span class="n">per</span> <span class="n">epoch</span>

   <span class="n">save</span> <span class="n">embeddings</span>
   <span class="n">visualize</span> <span class="n">embeddings</span> <span class="n">via</span> <span class="n">t</span><span class="o">-</span><span class="n">SNE</span>
</code></pre>

<h2>Submission</h2>
<p>Submit your jupyter notebook (or python script) via submitty. </p>
<p>The output of your code should be a file that contain the embedding of
each word. The first line should have only two values:</p>
<p>V d</p>
<p>where V is the Vocab size, and d the EMBED_DIM</p>
<p>Next, each line should contain:</p>
<p>WORD EMBEDDING_VECTOR</p>
<p>where WORD is a word from your vocab (not the index), and its embedding
vector. For example, if there are only two words in your vocab (say AA
and BB), and you are doing 3-dim embeddings, then the output file will
be:</p>
<pre class="code literal-block"><span></span><code><span class="mf">2</span> <span class="mf">3</span>
<span class="n">AA</span> <span class="o">-</span><span class="mf">1</span> <span class="o">-</span><span class="mf">0.3</span> <span class="mf">5</span>
<span class="n">BB</span> <span class="mf">2</span> <span class="mf">0.5</span> <span class="o">-</span><span class="mf">1</span>
</code></pre>

<p>After learning the representations, we will use the trained
vectors for some downstream tasks in future assignments.
For this assignment you should visualize your embeddings using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a>.</p>
<p>BTW, you should acknowledge the source of any code you use from the web.</p>
    </div>
    
        
        <script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script><script>
                renderMathInElement(document.body,
                    {
                        
delimiters: [
    {left: "$$", right: "$$", display: true},
    {left: "\\[", right: "\\]", display: true},
    {left: "$", right: "$", display: false},
    {left: "\\(", right: "\\)", display: false}
]

                    }
                );
            </script></article><!--End of body content--><footer id="footer">
            Contents Â© 2023         <a href="mailto:zaki@cs.rpi.edu">Mohammed J. Zaki</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../../../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script><!-- Global site tag (gtag.js) - Google Analytics --><script async src="https://www.googletagmanager.com/gtag/js?id=UA-11058802-1"></script><script>
window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());

gtag('config', 'UA-11058802-1');
</script>
</body>
</html>
