<h1>Assign6: Clustering: EM and Kernel KMeans</h1>
<p><strong>Due Date</strong>: Nov 30, before midnight (11:59:59PM, Alofi Time; GMT-11)</p>
<p>You will use the 
<a href="https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction#">Appliances energy prediction data set</a>.
You should ignore the first attribute, which is a date-time variable,
and you should also remove the last attribute, which is a duplicate of
the previous one. Also, the first attribute (after removing the
date-time variable), which denotes the
<strong>Appliances Energy Use</strong>, will NOT be used for clustering; instead we
will use it as the cluster label to assess the performance of the
clustering methods. Thus, ignoring "date", "Appliances", and "rv2", 
the remaining attributes will be used as the data for clustering.</p>
<p>Note that the <strong>Appliances Energy Use</strong> attribute takes values in the
range $[10,1080]$. However, for clustering, we will group these values
into 6 "true" clusters: $c_1$ is for values in the range $[10,40]$, 
$c_2$ for values $[50,50]$, $c_3$ for values $[60,60]$, 
$c_4$ for values $[70,90]$, $c_5$ for values $[100,160]$, and $c_6$ for
values $[170,1080]$.
This will result in cluster sizes
$|c_1| = 3094$, $|c_2| = 4368$, $|c_3| = 3282$, $|c_4|=3780$, 
$|c_5| = 3003$ and $|c_6| = 2208$.</p>
<hr>
<h1>CSCI4390/6390: Expectation Maximization Clustering</h1>
<p>Implement the Expectation-Maximization (EM) algorithm for clustering
(see Algorithm 13.3 in Chapter 13). Use the 'Appliances' attribute as
the true cluster label as described above, and use it for the purity-based clustering
evaluation (see below). Run with $k=6$ clusters. </p>
<p>For initializing the clusters, you can either choose random means as
described in the algorithm. Or you can shuffle the data, and pick the
first $k$ points as the mean. This has the advantage that the means are
always points in the dataset.</p>
<p>For practical purposes, you may want to use the <a href="https://blog.feedly.com/tricks-of-the-trade-logsumexp/">logexpsum trick</a> 
for expectation step, where you compute the log probabilities so that
you can deal with very small probability values, otherwise, you may find
that weights of a point for the clusters are zero.</p>
<p>As another practical point, you can get an error when inverting the covariance
matrix, so you should add a small ridge value $\lambda$ value along the
diagonal entries to make the matrix invertible. This can be considered
as a regularized estimate of the covariance matrix, i.e., $$\Sigma +
\lambda \mathbf{I}$$.</p>
<p>Your program output should consist of the following information:</p>
<ul>
<li>The final mean for each cluster</li>
<li>The final covariance matrix for each cluster</li>
<li>Size of each cluster, after assigning  each point to the cluster with highest posterior probability $P(c_i | x_j)$.</li>
<li>The 'purity score' for your clustering, computed as follows: Assume that $c_i$ 
   denotes the set of points assigned to cluster $i$ by the EM algorithm, and let $T_i$ 
   denote the true cluster id. Purity score is defined as:
   $$\frac{1}{n} \sum_{i=1}^k max_{j=1}^K \{c_i \cap T_j\} $$
   where $K$ is the true number of clusters, and $k$ is the input number
   of clusters to EM. See Eq(17.1) on pg 427 for more details on the purity measure.</li>
</ul>
<hr>
<h1>CSCI6390: Kernel K-Means</h1>
<p>Also implement the Kernel K-Means algorithm 13.2 on pg 341. Use only
the Gaussian kernel, but you'll have to choose the value of spread. </p>
<p>Your code must output the following information:</p>
<ul>
<li>Size of each cluster</li>
<li>The Purity for your clustering</li>
</ul>
<hr>
<h2>What to submit</h2>
<ul>
<li>For EM, write a script named as <strong>Assign7.py</strong>, which will be run as </li>
</ul>
<p>Assign6.py FILENAME k EPS RIDGE MAXITER</p>
<p>FILENAME is the datafile name, $k$ is the number of clusters to find,
 and EPS is the convergence threshold, RIDGE the $\lambda$ value for
 the ridge, and MAXITER is the maximum number of iterations to run (we
 need this since it may take a long time to converge for low EPS).
 Note that you should report the
 output for $k=6$, but your code should run for any input value of $k$.
 And you must output the purity for the given value of $k$ (which may
 not correspond to the true value $K=6$).</p>
<ul>
<li>For CSCI6390, for Kernel Kmeans, write a script <strong>Assign7-KK.py</strong>,
    which will be run as</li>
</ul>
<p>Assign7-KK.py FILENAME k EPS SPREAD</p>
<p>The parameters have the same meaning as given above, but SPREAD is
   the spread parameter for the Gaussian kernel.</p>
<p>Note that computing the full kernel matrix for 19K+
points will be memory intensive, so if you do not have enough memory,
one option is for you to repeatedly compute the required kernel values.
Alternatively, you can show results on at least 5000 points. 
However, you have to select these points using <strong>stratified sampling</strong>,
so that you choose a proportional number of points from each cluster
label. You can use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html">StratifiedShuffleSplit</a> from scikit-learn for
stratified sampling if you wish.</p>
<ul>
<li>Submit a PDF file named Assign7.pdf that should include your output </li>
<li>
<p>(just cut and paste the output from python).
 <strong>Failure the submit the PDF will result in lost points.</strong> </p>
</li>
<li>
<p>Submit the scripts and pdf file via submitty</p>
</li>
</ul>
<hr>
<h2>Policy on Academic Honesty</h2>
<p>You are free to discuss how to tackle the assignment, but all coding
must be your own. Please do not copy or modify code from anyone else,
including code on the web. Any students caught violating the academic
honesty principle will get an automatic F grade on the course and will
be referred to the dean of students for disciplinary action.</p>