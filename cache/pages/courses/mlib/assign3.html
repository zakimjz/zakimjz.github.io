<h1>Assign3: Protein Embeddings</h1>
<p><strong>Due Date</strong>: Feb 21, before midnight (EST 11:59:59PM)</p>
<p>In this assignment, you will implement the ProtVec embedding method
described in the paper
<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0141287">Continuous Distributed Representation of Biological Sequences for Deep
Proteomics and Genomics</a></p>
<p>Your code should implement the negative sampling approach to train the
embeddings. For training  the model, you can first use a small set of 1000 proteins
<a href="http://www.cs.rpi.edu/~zaki/MLIB/data/small_uniprot.txt">small_uniprot.txt</a>. 
Once your model is finalized you should train
it on the large set of 524532 protein sequences
<a href="http://www.cs.rpi.edu/~zaki/MLIB/data/uniprot-reviewed-lim_sequences.txt">uniprot-reviewed-lim_sequences.txt</a>. This data is from the paper
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6061698">Learned
protein embeddings for machine learning</a>.</p>
<p>You should compare with the default values used in the paper, namely
embedding dimensionality $d=100$, window size $w=25$, and k-mer/n-gram size $k=3$, and the number of negative samples per positive example $q=5$. </p>
<p>Here is the pseudo code for the overall method:</p>
<pre class="code literal-block"><span></span><code>   <span class="n">create</span> <span class="n">the</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">probability</span> <span class="n">distribution</span><span class="p">,</span> <span class="ow">and</span> <span class="n">word</span> <span class="n">to</span> <span class="n">index</span> <span class="p">(</span><span class="ow">and</span> <span class="n">reverse</span> <span class="n">mappings</span><span class="p">)</span> <span class="k">for</span> <span class="n">each</span> <span class="n">k</span><span class="o">-</span><span class="n">mer</span> <span class="ow">in</span> <span class="n">each</span> <span class="n">sequence</span> <span class="n">at</span> <span class="n">each</span> <span class="n">of</span> <span class="n">the</span> <span class="n">offsets</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">2.</span>

   <span class="n">write</span> <span class="n">a</span> <span class="n">function</span> <span class="n">to</span> <span class="k">return</span> <span class="n">a</span> <span class="n">batch</span> <span class="n">of</span> <span class="n">positive</span> <span class="ow">and</span> <span class="n">negative</span> <span class="n">pairs</span> <span class="kn">from</span> <span class="nn">all</span> <span class="n">of</span> <span class="n">the</span> <span class="n">sequences</span><span class="o">/</span><span class="n">offsets</span><span class="o">.</span>
   <span class="k">for</span> <span class="n">the</span> <span class="n">negative</span> <span class="n">sampling</span> <span class="n">use</span> <span class="n">the</span> <span class="n">cumsum</span> <span class="n">approach</span> <span class="n">to</span> <span class="n">sample</span> <span class="n">random</span> <span class="n">words</span> 

   <span class="n">define</span> <span class="n">NN</span> <span class="n">model</span><span class="p">:</span>
       <span class="n">init</span> <span class="n">function</span><span class="p">:</span>
           <span class="n">define</span> <span class="n">the</span> <span class="n">two</span> <span class="n">embeddings</span> <span class="n">layers</span> <span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

       <span class="n">forward</span> <span class="n">function</span><span class="p">:</span>
           <span class="nb">input</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">batch</span> <span class="n">of</span> <span class="n">target_words</span><span class="p">,</span> <span class="ow">and</span> <span class="n">context_words</span> 
           <span class="n">look</span> <span class="n">up</span> <span class="n">their</span> <span class="n">embeddings</span>
           <span class="n">compute</span> <span class="n">the</span> <span class="n">dot</span> <span class="n">product</span> <span class="n">between</span> <span class="n">corresponding</span> <span class="n">pairs</span>
           <span class="n">output</span> <span class="n">should</span> <span class="n">be</span> <span class="n">the</span> <span class="n">probability</span> <span class="n">of</span> <span class="n">that</span> <span class="n">pair</span> <span class="n">being</span> <span class="n">a</span> <span class="n">positive</span> <span class="n">pair</span> <span class="p">(</span><span class="n">via</span> <span class="n">sigmoid</span><span class="p">),</span>
           <span class="ow">or</span> <span class="n">leave</span> <span class="n">it</span> <span class="k">as</span> <span class="n">logits</span>

   <span class="n">Next</span> <span class="ow">is</span> <span class="n">the</span> <span class="n">boilerplate</span> <span class="n">code</span> <span class="k">for</span> <span class="n">training</span><span class="p">:</span>
   <span class="n">net</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
   <span class="n">send</span> <span class="n">net</span> <span class="n">to</span> <span class="n">GPU</span>
   <span class="n">loss_func</span> <span class="o">=</span> <span class="n">BCE</span> <span class="n">loss</span> <span class="p">(</span><span class="k">with</span> <span class="n">logits</span><span class="p">)</span>
   <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span>

   <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">epochs</span>
       <span class="k">for</span> <span class="n">target_words</span><span class="p">,</span> <span class="n">context_words</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">batches</span>
          <span class="n">send</span> <span class="n">batch</span> <span class="n">data</span> <span class="n">to</span> <span class="n">the</span> <span class="n">GPU</span>
          <span class="n">net</span><span class="o">.</span><span class="n">zero_grad</span>
          <span class="n">preds</span> <span class="o">=</span> <span class="n">net</span> <span class="p">(</span><span class="n">target_words</span><span class="p">,</span> <span class="n">contex_words</span><span class="p">)</span>
          <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
          <span class="n">loss</span><span class="o">.</span><span class="n">backward</span>
          <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span>

      <span class="nb">print</span> <span class="n">total</span> <span class="n">loss</span> <span class="n">per</span> <span class="n">epoch</span>

   <span class="n">save</span> <span class="n">embeddings</span>
   <span class="n">visualize</span> <span class="n">embeddings</span> <span class="n">via</span> <span class="n">t</span><span class="o">-</span><span class="n">SNE</span>
</code></pre>

<h1>Submission</h1>
<p>Submit your jupyter notebook (or python script) via submitty. </p>
<p>The output of your code should be a file that contain the embedding of
each word. The first line should have only two values:</p>
<p>V d</p>
<p>where V is the Vocab size, and d the EMBED_DIM</p>
<p>Next, each line should contain:</p>
<p>WORD EMBEDDING_VECTOR</p>
<p>where WORD is a word from your vocab (not the index), and its embedding
vector. For example, if there are only two words in your vocab (say AA
and BB), and you are doing 3-dim embeddings, then the output file will
be:</p>
<pre class="code literal-block"><span></span><code><span class="mf">2</span> <span class="mf">3</span>
<span class="n">AA</span> <span class="o">-</span><span class="mf">1</span> <span class="o">-</span><span class="mf">0.3</span> <span class="mf">5</span>
<span class="n">BB</span> <span class="mf">2</span> <span class="mf">0.5</span> <span class="o">-</span><span class="mf">1</span>
</code></pre>

<p>After learning the representations, we will use the trained
vectors for some downstream tasks in future assignments.
For this assignment you should visualize your embeddings using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a>.</p>
<p>BTW, you should acknowledge the source of any code you use from the web.</p>