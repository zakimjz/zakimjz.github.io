<h1>Assign1: Transcription Factor Binding via MLP</h1>
<p><strong>Due Date</strong>: Feb 1, before midnight (EST 11:59:59PM)</p>
<p>Transcription Factors (TFs) are proteins that bind to the DNA and help
regulate gene transcription. The TFs have to recognize some "motif" on the
DNA upstream from the gene, and DNA accessibility also plays a role.</p>
<h2>MLP model</h2>
<p>In this assignment you'll modify write an MLP model the predict whether a
segments of the human chromosome 22 (Chr22) contain the binding sites for the JUND
TF.  You can modify the mlp notebook I shared with you to work on this
problem. You need to have at least one hidden layer. You have to compute a
weighted loss, and include accessibility information in your model, as
described below.</p>
<h2>Dataset</h2>
<p>The data comprises 101 length segments from Chr22, with each position a
one-hot vector denoting one of the four bases (A, C, G, T). Thus, each
element of the input is 2d with size $101 \times 4$. Each such element has
a target label $0$ or $1$, indicating whether the TF binds to that segment or not. 
The data also includes a weight per input element, since there are only a
few binding sites (0.42%), so that you'd obtain an accuracy of 99.58% just
by predicting there are no binding sites. This means you have to use the
weights to discount the losses for label $0$ and enhance the losses for
label $1$ items. 
Finally, there is an array of values, one per input element, that also
indicates the chromosome accessibility for that segment.</p>
<p>The data is split into training, validation and testing sets. Each set
contains the following files:</p>
<ul>
<li>shard-0-X.joblib: the set of 101 x 4 input elements</li>
<li>shard-0-y.joblib: the true labels: 0 or 1</li>
<li>shard-0-w.joblib: weight per input element</li>
<li>shard-0-a.joblib: accessibility value per input element</li>
</ul>
<p>You can read these files by using <strong>joblib.load</strong> function, which will
populate a numpy array. For example</p>
<pre class="code literal-block"><span></span><code><span class="n">X</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;shard-0-X.joblib&#39;</span><span class="p">)</span>
</code></pre>

<p>will results in a numpy array X, which you can then convert to torch tensor,
and so on.</p>
<p>You can download the data as <a href="https://www.cs.rpi.edu/~zaki/MLIB/data/TF_data.zip">TF_data.zip</a>. Unzip it to create the train, valid, test directories.</p>
<h2>Details</h2>
<p>First, you must write a dataset class that can load the data from the
different directories (train_dataset, valid_dataset, test_dataset).
Your class will have the following structure:</p>
<pre class="code literal-block"><span></span><code><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">JUND_Dataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_dir</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;load X, y, w, a from data_dir&#39;&#39;&#39;</span>        
        <span class="nb">super</span><span class="p">(</span><span class="n">JUND_Dataset</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># load X, y, w, a from given data_dir</span>
        <span class="c1"># convert them into torch tensors</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;return len of dataset&#39;&#39;&#39;</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;return X, y, w, and a values at index idx&#39;&#39;&#39;</span>
</code></pre>

<p>Once you have created the dataset class it can be passed to the DataLoader
class to get batches like we did for MNIST.</p>
<p>For the MLP model, the only difference is that the 2d input ($101\times 4$)
will have to be flattened into 404d vector. Then pass it to the hidden
layer, and apply relu activation. 
You can also try to add in a dropout layer at this point.
However, before feeding the output of the
hidden layer to the output layer, you must concatenate the accessibility
value. So if you are using hidden dimension of 128, then after concatenating
the accessibility value, it will become a 129d vector, which should be fed
to the final output layer of size 1, since we have a binary class/label.</p>
<p>The only other issue is the loss function. You should use
<a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy_with_logits.html">binary_cross_entropy_with_logits</a> with weight set to the weights per
input element. Check out the documentation for details.</p>
<p>You need to train the model on the training data, and use the validation
data to select how many epochs you want to use and to choose the hidden
dimension. Use the weighted prediction accuracy as the evaluation metric.
That is, sum of the weights of the correct predictions divided by the total
weight across all the input elements. Finally, report the weighted accuracy
on the test data.</p>
<h2>What to submit</h2>
<ul>
<li>Upload your jupyter notebook on <a href="https://submitty.cs.rpi.edu/courses/s22/csci4969/gradeable/Assign1">submitty</a></li>
<li>The notebook must have output values for the final test accuracy.</li>
<li>Do not submit the data file or directories. </li>
</ul>